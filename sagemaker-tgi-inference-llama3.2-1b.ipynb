{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30da0cf-88c1-4cdd-8240-56b45c667e6c",
   "metadata": {},
   "source": [
    "## Deploy Llama 3.2 1B on AWS Inferentia and SageMaker using HuggingFace TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26941c1-ca40-459f-bc11-6a7c612609f5",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to deploy [Meta Llama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) model using [Hugging Face Text Generation Inference (TGI) Deep Learning Container on Amazon SageMaker](https://huggingface.co/docs/optimum-neuron/en/guides/neuronx_tgi).\n",
    "\n",
    "TGI is an open source, high performance inference library that can be used to deploy large language models from Hugging Faceâ€™s repository in minutes. The library includes advanced functionality like model parallelism and continuous batching to simplify production inference with large language models like Mistral, LLaMa, StableLM, and GPT-NeoX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbddc3-cef9-417c-9500-a55da0916437",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Setup\n",
    "Install the SageMaker Python SDK, huggingface_hub and transformers.\n",
    "\n",
    "First, make sure that the required version are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a86338-af9b-4e68-9f83-c5d8db30960d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.216.0\" --upgrade --quiet\n",
    "!pip install \"huggingface_hub==0.24.6\" --upgrade --quiet\n",
    "!pip install \"transformers==4.45.2\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168850d-11cb-4084-abc1-1ac1a59b0f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Setup account and role\n",
    "\n",
    "Then, we import the SageMaker python SDK and instantiate a sagemaker_session which we use to determine the current region and execution role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf6c68-d00b-4ed0-8ee1-c5156fc9712c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ad4f2-0b05-4803-ad6e-a5f38555e4c9",
   "metadata": {},
   "source": [
    "The following code defines the `docker image uri` and `model_id`.\n",
    "\n",
    "`llm_image` is set to the image URI for the Hugging Face Large Language Model (LLM) inference container.\n",
    "\n",
    "`model_id` is a Hugging Face model repository that has the model weights. In this case, it contains pre-compiled neuron artifacts. This is described in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500087e-ce77-4e42-aa08-93848adb81d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    " \n",
    "# Define the llm image uri\n",
    "llm_image=\"763104351884.dkr.ecr.\"+sess.boto_region_name+\".amazonaws.com/huggingface-pytorch-tgi-inference:2.1.2-optimum0.0.25-neuronx-py310-ubuntu22.04-v1.0\"\n",
    "\n",
    "# To save deployment time, we used a pre-compiled Meta-Llama-3.2-1B model\n",
    "model_id=\"cszhzleo/Meta-Llama-3.2-1B-Instruct-nc2-bs1-token1024-neuron-220\"\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ede85-fe78-40ae-8548-e3ec79a8423e",
   "metadata": {},
   "source": [
    "To deploy models on  [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls), one needs to compile the model to a NEFF(Neuron Executable File Format) file that is loaded onto the Neuron devices.\n",
    "\n",
    "For the purpose of this workshop to save time, we have pre-compiled [Meta Llama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) and uploaded the resultant artifacts to to the [HuggingFace model hub](`cszhzleo/Meta-Llama-3.2-1B-Instruct-nc2-bs1-token1024-neuron-220`).\n",
    "\n",
    "Here, we use [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/en/index). Optimum Neuron is the interface between the HuggingFace Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls).\n",
    "\n",
    "We use the following command to compile and export model to `OUTPUT_PATH`:\n",
    "\n",
    "```\n",
    "optimum-cli export neuron  --model meta-llama/Llama-3.2-1B-Instruct --batch_size 1 --sequence_length 1024 --num_cores 2 --auto_cast_type fp16  <OUTPUT_PATH>\n",
    "```\n",
    "\n",
    "To upload compiled model to HuggingFace, run the following commands:\n",
    "\n",
    "```\n",
    "huggingface-cli login # Login HuggingFace, user needs to prepare a user access tokens with write permissions\n",
    "\n",
    "huggingface-cli upload <HF_MODEL_ID> <OUTPUT_PATH> # Upload complied model file\n",
    "\n",
    "```\n",
    "\n",
    "For more details, refer the [guide on exporting models to AWS Trainium and Inferentia using Optimum Neuron](https://huggingface.co/docs/optimum-neuron/en/guides/export_model).\n",
    "\n",
    "\n",
    "In practice, you can skip this step and just use the `modelid` on the HuggingFace model hub. The framework maintains a cache of pre-compiled artifacts (NEFF files) of popular models and configurations in a public repository. When you specify a `modelid`, TGI looks up the [Neuron Model Cache](https://huggingface.co/aws-neuron/optimum-neuron-cache) and uses the cached artifacts to deploy the model.\n",
    "\n",
    "A cached configuration is defined through a model architecture (Llama3), model size (8B), neuron version (2.18), number of inferentia cores (2), batch size (4), and sequence length (4096).\n",
    "\n",
    "This means that when deploying models with an architecture based on Llama3 and a configuration for which Neuron compiled artifacts exist; there will be no need to re-compile your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481728af",
   "metadata": {},
   "source": [
    "### Configure variables for `TGI`\n",
    "\n",
    "The next step is to define certain configurations that will be used to deploy the model.\n",
    "- `HF_NUM_CORES`: the number of Neuron cores across which the model will be partitioned (tensor parallel degree)\n",
    "- `HF_BATCH_SIZE`: the batch size to be used for inference\n",
    "- `HF_SEQUENCE_LENGTH`: the total sequence length (input + output) of requests to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82234c08-608d-44b9-b54b-c3ca44c46f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    " \n",
    "# sagemaker config\n",
    "instance_type = \"ml.inf2.xlarge\"\n",
    "health_check_timeout=2400 # additional time to load the model\n",
    "volume_size=100 # size in GB of the EBS volume\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"HF_NUM_CORES\": \"2\", # number of neuron cores\n",
    "    \"HF_BATCH_SIZE\": \"1\", # batch size used to compile the model\n",
    "    \"HF_SEQUENCE_LENGTH\": \"1024\", # length used to compile the model\n",
    "    \"HF_AUTO_CAST_TYPE\": \"fp16\",  # dtype of the model\n",
    "    \"MAX_BATCH_SIZE\": \"1\", # max batch size for the model\n",
    "    \"MAX_INPUT_LENGTH\": \"256\", # max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"1024\", # max length of generated text\n",
    "    #\"HF_TOKEN\": HfFolder.get_token(), # pass the huggingface token\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d814e-8aa0-4b1c-b52c-98ad5b617cf1",
   "metadata": {},
   "source": [
    "\n",
    "### Create the Hugging Face Model\n",
    "\n",
    "Next we configure the model object by specifying the `image_uri` of the managed TGI container, and the execution `role` for the endpoint. Additionally, we specify a number of environment variables defined above, including the `HF_MODEL_ID` which corresponds to the model from the HuggingFace Hub that will be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e95b2-fd3b-4672-8e7d-0ceb2f92cba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24906c4d-3efa-425e-b02d-f288589f7005",
   "metadata": {},
   "source": [
    "\n",
    "### Creating a SageMaker Endpoint\n",
    "\n",
    "Next we deploy the model by invoking the `deploy()` function.\n",
    "\n",
    "To efficiently deploy and run large language models, it is important to choose an appropriate instance type that can handle the computational requirements. Here we use an `ml.inf2.xlarge` instance which come with 2 neuron cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d4970-1fe3-43be-847f-4af6ea108650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model._is_compiled_model = True\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  volume_size=volume_size,\n",
    "  endpoint_name=\"llama-32-1b-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8354e",
   "metadata": {},
   "source": [
    "**The above step takes about 10-15 minutes.**\n",
    "While you wait for the model to be deployed, you can read the below resources -\n",
    "\n",
    "- [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)\n",
    "- [AWS Inferentia2](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/inf2-arch.html)\n",
    "- [Amazon SageMaker Realtime Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html)\n",
    "- [Amazon SageMaker with HuggingFace Optimum Neuron](https://huggingface.co/docs/optimum-neuron/en/guides/sagemaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25933b9e-ba1c-40f8-b5aa-117e189b4760",
   "metadata": {},
   "source": [
    "\n",
    "### Running Inference\n",
    "\n",
    "Once the endpoint is up and running, we can evaluate the model using the predict() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7af90-f049-4448-b3e1-3c06806f3f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " \n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me an interesting fact about AWS?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    " \n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"max_new_tokens\": 768,\n",
    "}\n",
    " \n",
    "res = llm.predict({\"inputs\": prompt, \"parameters\": parameters})\n",
    "print(res[0][\"generated_text\"].strip().replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d15d6-f3d0-41db-8670-0e7cd97180e1",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "Streaming responses from LLMs can significantly improve user experience by reducing wait times and providing real-time feedback. Here is the inference with streaming mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a22565-6da0-4825-a6e6-13c4f97143d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "body = json.dumps({\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":768}, \"stream\": True})\n",
    "\n",
    "resp = sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=llm.endpoint_name,\n",
    "    Body=body,\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    ")\n",
    "text = \"\"\n",
    "for e in resp['Body']:\n",
    "    tok = e['PayloadPart']['Bytes'].decode('utf-8')\n",
    "    if tok.startswith('data'): \n",
    "        try:\n",
    "            tok = json.loads(tok[5:])\n",
    "            print(tok['token']['text'], end='')\n",
    "        except Exception as e:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8736f16-12e4-4dc5-bed2-bd945059687d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Cleaning Up\n",
    "\n",
    "After you've finished using the endpoint, it's important to delete it to avoid incurring unnecessary costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300d87c-9b35-459d-aa55-fd1e9b4f23f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
